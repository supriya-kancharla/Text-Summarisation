# Text-Summarisation
These days, nobody has the time to read a docu- ment or review cover to cover in order to under- stand its full content. In order to save time and make life easier for individuals, there is a high need for automatic text summary. The number of people using the internet grows every year. A growing amount of data is uploaded to the Inter- net every second as there are more and more users. Online shopping has reached previously unimag- inable heights as a result of the widespread use of the Internet. Instead of making in-store purchases, everyone wants to place online orders. Before buy- ing a product, a consumer will check for reviews written by prior buyers as the very first thing. It is becoming more and more difficult for a con- sumer to sort through numerous reviews of vari- ous products of a given type and choose the finest one. In order to assist the consumer in selecting the best product from the group, it is necessary to summarise these reviews as completely as possi- ble. The method of condensing text without losing meaning. As a result, text summarization enters the picture and could lead to a review summary. For our research, we are using a dataset of Ama- zon reviews. Numerous NLP tasks, including text cat- egorization, information retrieval, legal text summarization, mainstream media summariza- tion, and headline creation, may benefit from text summarization. Additionally, the creation of summaries may be integrated into these systems as a step in the procedure, resulting in a smaller document. The accessibility of documents has increased, necessitating in-depth research in the field of NLP for automatic text summarization. It is the process of creating a succinct, vivid summary without the assistance of a human being while preserving the original text’s meaning. Condensing a lengthy text into a brief one while keeping the primary concept intact is called text summarization. Massive amounts of data will soon be produced every day as information tech- nologies advance and change quickly, especially for mobile Internet. Automated summarization is therefore becoming more and more important in the age of massive amounts of information.

I used Vanilla Transformers to summarise the amazon food reviews given by customers. This model can now comprehend the context of the entire review to make it easy for the customer to read shorter summaries of the reviews. Later monitored the training of the model using cross-entropy loss and accuracy. 
# Data Preparation
The data collected from Amazon reviews is used in this project. The data consists of 100,000 re- views up to October 2012 and cover a period of more than ten years. Reviews contain information on the product and the user, as well as ratings and simple language. Additionally, all other Ama- zon categories’ reviews are included. Here for this project we are using only columns containing Text and Summary. In the Text column there is the cus- tomer’s review with long sentences or small para- graphs. In the Summary column there is a sen- tence with at most 5 words describing the entire text context. The dataset is split into 90k of data training and 10k data for validation and testing re- spectively.
# Model
The vanilla transformer consists of a stack of L identical units called an encoder and a decoder. A position-wise feed-forward network and a multi- head self-attention module make up the majority of each encoder block (FFN). A residual connec- tion is used around each module to help develop a deeper model, which is then followed by a Layer Normalisation module. Decoder blocks, in contrast to encoder blocks, also insert cross-attention modules between the position-wise FFNs and the multi-head self-attention modules. The decoder’s self-attention modules have also been modified to stop each position from attending to earlier posi- tions. 
# Training
The Vanilla transformer model is trained with se- quences from the pre-processed dataset. The re- view is first embedded into 128 dimensional vec- tor for each token and positional embedding is added to sequence embedding. This embedding is then passed to the encoder block along with the encoder mask. The encoder mask is to indicate the non-padded tokens. The model to be trained in this project is built with 3 layers of encoder and de- coder stacks and each stack has multi-head attention with 4 heads. The maximum sequence length (seq length) that can be fed into the model in a single step is 100. The dimension of embed- dings (emb dim) for each token is also set to be 128, which means after a forward pass through the model each of those tokens in the input sequence will be embedded into a 128 dimension vector. The output of the encoder is then subsequently passed to the decoder along with the summary em- bedding summed up with positional encoding, de- coder look-ahead mask and decoder mask. The de- coder look-ahead mask is used to mask out the fu- ture output tokens. The output from the decoder blocks are then passed to the fully connected layer with the nodes equal to the size of the vocab. The size of the vocab in this project is 54861.
The model is trained with Adam optimizer with a custom learning rate. The custom learning rate increases from 0.0001 to 0.001 as the training steps increase to 5K and starts to decay. The loss function used is cross entropy loss; training and validation loss are calculated after every epoch and the checkpoints are saved after every 5 epochs. Accuracy is calculated as word wise comparison of the predicted summary sequence and the origi- nal summary. The accuracy is also monitored after every epoch.
